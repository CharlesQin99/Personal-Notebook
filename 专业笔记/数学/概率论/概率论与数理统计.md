# 概率论与数理统计

研究和揭示 **<u>随机现象统计规律</u>**

> **拉普拉斯**做过**拿破仑**的数学老师

## 随机试验

**通过研究随机试验来研究随机现象**，记为E

需要满足**三个条件**：

- **试验可以在相同条件下<u>重复</u>进行**
- **每次试验结果可能不止一个，并且能事先明确试验的所有可能结果**
- **试验之前不能确定哪一个结果会出现**

### 样本空间

**样本空间**：随机试验中·**所有可能结果组成的<u>集合</u>**，记为S

**样本点**：随机试验的每个可能结果，记为e

> 样本空间是由所有样本点组成的集合

### 随机事件

**随机事件**：**<u>满足某些条件的样本点</u>**组成的集合 

 **必然事件**：样本空间S包含所有样本点，每次试验必发生

**不可能事件**：不包含任何样本点的空集

#### 事件间的关系

![](PIC\0.png)

#### 运算法则

**A - B**：**<u>当且仅当A发生，B不发生</u>**

![](PIC\1.png)

![](PIC\2.png)

##### 德摩根律

![](PIC\3.png)

## 频率与概率

概率：随机事件A**发生的可能性大小**的度量

频率：![4](PIC\4.png)

无限次试验后，频率无限接近概率

![](PIC\5.png)

![](PIC\6.png)

## 古典概型（等可能概型）

**排列**

![](PIC\7.png)

**组合**

取出m和取出n-m是一样的

![](PIC\8.png)

**古典概型定义**：

若随机试验E满足：

- **样本空间只含有有限个样本点**
- **<u>每个样本点发生的可能性相同</u>**

称此随机试验的概率模型为**<u>等可能概型</u>**，也称为**古典概型**

![](PIC\9.png)

### 生日问题

![](PIC\10.png)

**超几何分布**![11](PIC\11.png)

## 几何概型

**定义：**

若随机试验E满足：

- **样本空间S是一个<u>*可度量*</u>的几何区域**
- **每个样本点出现的概率相等，即等可能地落入区域中**

![](PIC\12.png)

**几何概型和古典概型的区别**

![](PIC\13.png)

求解几何概型时，**<u>核心是画出题目中所涉及的区域</u>**

## 条件概率

**重点：<u>样本空间的变化</u>**

![](PIC\14.png)

![](PIC\15.png) 

## 乘法公式

![](PIC\17.png)

![](PIC\16.png)

![](PIC\18.png)

## 全概率公式

找到**<u>样本空间的一个划分</u>**（**完备事件组**），且两两**<u>互不相容</u>**，以此来计算**<u>合事件的概率</u>**

> **划分一般不唯一，计算时候需要合理划分**

![](PIC\19.png)

![](PIC\20.png)

## 贝叶斯公式

**由“果”求“因”**，**即当A发生后，对原来Bi的概率有了新的认识**

![](PIC\82.png)

![](PIC\83.png)

**<u>*新证据不能直接凭空决定你的看法，而是更新你的先验看法，以此来计算后验概率*</u>**

> **公式 = 乘法公式 / 全概率公式**

![](PIC\21.png)

## 独立性

如果独立的话, 条件概率就不用求了**P（B|A) = P(B)**

![](PIC\23.png)

- **两个事件相互独立与互不相容<u>不能同时成立</u>**
- **必然事件与不可能事件与任意事件相互独立**

![](PIC\24.png)

**<u>多个事件ABC 相互独立 比 两两独立 条件更强</u>**

## 随机变量

**目的**：借助<u>**函数**</u>来**<u>研究随机现象</u>**

> **<u>随机变量就是一个定义域为*样本空间*的*实值单值函数*</u>**

![](PIC\25.png)

![](PIC\26.png)

### 离散型随机变量

**DRV：Discrete random variable**

随机变量X可能取到的值是**有限个，或者可列无限多个**

![](PIC\27.png)

#### 0-1分布

X只可能取到0和1

![](PIC\28.png)

#### 二项分布

瑞士著名数学家**雅克-伯努利**发现的，

伯努利试验：**将事件E独立重复地进行n次，称为n重伯努利试验**

- **E只有两个结果**
- **每次E中p都不变**
- **试验独立重复n次**

**二项分布**：**<u>*可以表示成n个0-1分布的和，就是 n重伯努利试验*</u>**。以X表示n重伯努利试验中**<u>事件A发生次数</u>**，求X的分布律

![](PIC\29.png)

![](PIC\30.png)

可以解决类似实际问题：

![](PIC\31.png)

##### 计算技巧

![](PIC\192.png)

#### 泊松分布

![](PIC\32.png)

##### 泊松定理

证明了**n趋于无穷大**的时候，**<u>二项分布可以用参数 入 = np 的泊松分布来近似</u>**

![](PIC\34.png)

![](PIC\33.png)

**小结**

![](PIC\35.png)

#### 几何分布和超几何分布

**定义：每次试验成功的概率是p，<u>第一次成功所需的试验次数</u>X，就是服从参数为p的<u>几何分布</u>**

只有一个参数**p**

![](PIC\36.png)

**超几何分布**：**N 产品总数，M 次品数，任取 n 件  ，随机变量X为取到次品数服从<u>超几何分布</u>**

![](PIC\37.png)

###  分布函数

**CDF**：**分布函数，也叫累积分布函数 (cumulative distribution function)** 

概率密度函数的积分，能**完整描述一个实<u>随机变量X的*概率分布*</u>**

![](PIC\38.png)

**F（x）本质就是一个<u>概率</u>，值域为[0 , 1]**，是一个**分段函数**

分布函数可以**应用在离散或者非离散的随机变量上**

几何含义：

![](PIC\39.png)

分布函数的**充要条件**：

- **单调不减**
- ![](PIC\40.png)
- **右连续**![](PIC\41.png)

### 连续型随机变量

**CRV：Continuous random variable**

#### 概率密度函数

**<u>PDF：probability density function</u>**

数学中，**连续型**随机变量的概率密度函数是一个**描述这个随机变量的输出值，在某个确定的取值点<u>附近</u>的可能性的函数。即用某一区间上的积分来刻画随机变量落在这个区间中的概率**

![](PIC\67.png)

> 1. PDF是**连续变量**特有的，PMF是**离散随机变量**特有的。
> 2. **PDF**的取值**<u>本身不是概率</u>**，它是**只有对连续随机变量的取值进行积分后才是概率**，也就是说对于连续值确定它在某一点的概率是没有意义的，
> 3. PMF的**取值本身代表该值的概率**

![](PIC\42.png)

![](PIC\43.png)

**概率密度函数的性质**：

- 非负性，可积性
- ![](PIC\44.png)
- ![](PIC\45.png)
- 若f(x)在x0**连续**，**<u>分布函数的导数 = 概率密度函数</u>**![](PIC\46.png)

连续型随机变量的**特性**：

- **<u>分布函数F(x)一定是连续的</u>**
- 若f(x)在x0**连续**，**<u>分布函数的导数 = 概率密度函数</u>**

- 在某一点的概率为0![](PIC\47.png)


#### 均匀分布

定义：

![](PIC\48.png)

分布函数：

![](PIC\49.png)

#### **正态分布**

##### 也叫**高斯分布**，参数是**μ(米尤)，σ(西玛)** 

![](PIC\50.png)

##### **性质**：

- **对称性**，**对称轴为u**![](PIC\51.png)
- **最大值**![](PIC\52.png)
- **凹凸性**![](PIC\53.png)

> **拐点的充分条件：**
>
> 又称**反曲点**，在数学上指**<u>改变曲线向上或向下方向</u>**的点，直观地说**拐点是使切线穿越曲线的点**
>
> 1、设f(x)在(a,b)内二阶可导，x0∈(a,b) ，则f''(x0)=0， 若在x0两侧附近f''(x0)异号，则点(x0,f(x0) )为曲线的拐点。否则(即f''(x0) 保持同号，(x0,f(x0) )不是拐点。
>
> 2、函数图像上的某点**使函数的二阶导数为零，且三阶导数不为零时，这点即为函数的<u>拐点</u>**。
>
> (1)如果在(a,b)内**f''(x)>0**,则**曲线y=f(x)在(a,b)上是凹**的
>
> (2)如果在(a,b)内**f''(x)<0**,则曲线**y=f(x)在(a,b)上是凸**的
>
> 连续曲线**凹与凸的分界点称为曲线的拐点**

图像：

![](PIC\54.png)

![](PIC\55.png)

**伽尔顿板**

![](PIC\225.png)

![](PIC\226.png)

##### 标准正态分布

![](PIC\56.png)

![](PIC\57.png)

**标准正态分布通过<u>*查表*</u>可以知道分布函数在不同点的函数值**

##### 3σ**法则**

**<u>距离均值（期望）1-3个标准差的范围内包含了大部分数据</u>**

**0.1->2.1->13.6->34.1**

1. 68.2%
2. 95.4%
3. **99.7%**

![](PIC\339.png)

**6σ质量管理**

6σ的**质量水平**，**<u>追求零缺陷生产，99.999998%是无缺陷的</u>**

![](PIC\340.png)

##### 正态分布相关计算

**<u>技巧：可以将一个正态分布的随机变量转换为一个标准正态分布的，方便计算</u>**

![](PIC\58.png)

**推论**：

![](PIC\59.png)

#### 指数分布

![](PIC\60.png)

**分布函数**

![](PIC\61.png)   

**无记忆性**

![62](PIC\62.png)

#### **帕累托分布**

**Pareto Distribution**

即**<u>28法则</u>**，核心思想：20%的人将占有80%的社会财富

![](PIC\341.png)

​																							**“长尾模型”**

帕累托分布是以意大利经济学家维**弗雷多·帕雷托**命名的。 是从大量真实世界的现象中发现的**幂次定律分布**。

帕累托因对**意大利20%的人口拥有80%的财产的观察而著名**，后来被约瑟夫·朱兰和其他人概括为**帕累托法则（80/20法则），后来进一步概括为帕累托分布的概念。**

> 帕累托分布可以归纳为一个非常简洁的表述：**通过市场交易，20%的人将占有80%的社会财富，如果交易可以不断进行下去，那么，“在因和果、努力和收获之间，普遍存在着不平衡关系，典型的情况是：80%的收获来自20%的努力；其他 80%的力气只带来20%的结果”。**
>
> 越来越多的行业**，利润都集中在少数几家公司手里**。
>
> 举例来说，在过去20年里，75%的美国行业集中程度都升高了。1978年，**最赚钱的100家公司所得利润占所有上市公司总利润的48%，但到2015年，这一数字甚至升至84%**。（详见边栏“少数公司日益增强的权力”）所谓新经济的成功故事在某种程度上要对此负责——**<u>*平台企业迅速发展，其网络效应往往会带来竞争优势*</u>**，从而迅速**将高斯分布转变成帕累托分布**，金·卡戴珊和Instagram就是典型案例。

### 随机变量函数分布

**<u>随机变量的函数也是随机变量</u>**

![](PIC\63.png)

1.**分布函数求导法**

利用**<u>复合函数</u>**求导，**<u>*先求出分布函数FY(y)的表达式，然后求导得出fY(y)*</u>**

![](PIC\64.png)

2.**公式法**

y=g(x)是x的**<u>*单调可导函数，即 g(x)的导数恒>0或<0*</u>**的时候才可以用

其中**h(y)** = x是g(x) = y的**反函数**，h函数就是用y表达x

![](PIC\65.png)

![](PIC\66.png)

### 二维随机变量

![](PIC\68.png)

![](PIC\69.png)

#### 联合分布函数

F可以看成事件A和事件B同时发生的概率

![](PIC\70.png)

**性质：**

- 单调性![](PIC\71.png)
- 有界性 
- 极限![](PIC\72.png)
- 

![](PIC\73.png)

#### 二维离散型随机变量

![](PIC\74.png)

例：

**矩阵法求解**

![](PIC\112.png)

![](PIC\113.png)

#### 二维连续型随机变量

![](PIC\75.png)

![](PIC\76.png)

![](PIC\77.png)

例：

![](PIC\78.png)

![](PIC\79.png)

n维随机变量

![](PIC\80.png)

![](PIC\81.png)

#### 边缘分布

简而言之，就是**<u>X和Y分别考虑各自的分布函数</u>**

**其实就是根据联合分布函数<u>求极限</u>的问题**

![](PIC\84.png)

![](PIC\85.png)

例：

![](PIC\86.png)

1.**<u>确定定义域</u>**

![](PIC\87.png)

2.对y作积分

![](PIC\88.png)

3.同理求**Y的边缘概率密度函数**

![](PIC\89.png)

#### 条件分布

例：

![](PIC\90.png)

![](PIC\92.png)

![](PIC\93.png)

![](PIC\94.png)

![](PIC\95.png)

![](PIC\96.png)

![](PIC\97.png)

### 相互独立的随机变量

**离散型**

![](PIC\98.png)

**联合分布律 = 边缘分布律的乘积（分布函数也可以，但是离散型一般不用）**

![](PIC\99.png)

例：

![](PIC\100.png)

![](PIC\101.png)

![](PIC\102.png)

**连续性**

![](PIC\103.png)

**<u>联合概率密度函数</u> = <u>边缘概率密度函数</u>的乘积**

同理**分布函数**

![](PIC\107.png)

例：

![](PIC\105.png)

![](PIC\106.png)

可以推广到n维随机变量

![](PIC\108.png)

### 二维正态分布

![](PIC\109.png)

![](PIC\110.png)

性质：

- **二维正态分布的两个边缘分布也是两个正态分布**
- **对于不同的p，他们的边缘正态分布是相同的**
- **p=0时，两个边缘分布X,Y是相互独立的**

推广：

![](PIC\122.png)

### 二维均匀分布

![](PIC\111.png)

### 二维连续随机变量运算

和运算分布：

![](PIC\114.png)

推导：利用分布函数然后求导，叫做**卷积公式**

![](PIC\115.png)

![](PIC\116.png)

#### e的性质

e=(1+1/n)^n

![](PIC\119.png)

![](PIC\117.png)

证明：

![](PIC\121.png)

![](PIC\120.png)

**斯特林公式**：

![](PIC\118.png)

**商和积运算的分布**：

核心：

![](PIC\123.png)

#### 商的分布的概率密度

![](PIC\124.png)

#### 积的分布的概率密度

![](PIC\125.png)

## 最大值最小值分布

求max：

![](PIC\126.png)

利用逆事件求min：

![](PIC\128.png)

![](PIC\129.png)

**概率密度**：

最大值：

![](PIC\127.png)

最小值：

![](PIC\130.png)

**例题**：

![](PIC\132.png)

![](PIC\131.png)

## 随机变量的数字特征

数字特征：由随机变量分布确定，能**反映其某一方面特征的常数**。

例如：**数学期望，方差，相关系数，矩**

解决问题：

- **随机变量不易求，不能求的时候**
- 只想要知道随机变量某些特征的常数，**不感兴趣其概率分布**
- **<u>知道其分布类型，只需要知道其数字特征后就能确定概率分布</u>**

![](PIC\133.png)

### 数学期望

数学期望：**<u>*刻画了随机变量X取值的平均水平*</u>**

**离散型随机变量**：类似**加权平均**，代表**<u>*X取值的平均水平*</u>**

级数必须决定收敛，否则数学期望可能不存在

![](PIC\134.png)

**连续型随机变量**：

![](PIC\135.png)

运算流程：

1. **分布函数表达**
2. **求导得到概率密度函数**
3. **积分得到期望**

**泊松分布**的**期望就是参数λ**

![](PIC\136.png)

**均匀分布**的期望是(a+b)/2

![](PIC\137.png)

#### 一维随机变量

![](PIC\138.png)

![139](PIC\139.png)

例题：

**离散型**

![](PIC\140.png)

泊松分布（可列无限多个）

![](PIC\141.png)

**连续型**

![](PIC\142.png)

![143](PIC\143.png)

#### 二维随机变量

![]()![144](PIC\144.png)

![](PIC\145.png)

#### 性质

![](PIC\146.png)

### 方差

期望描述了随机变量的平均大小

**<u>方差就用来描述了随机变量X离期望E的偏离程度，即*描述了X取值的分散程度*</u>**

![](PIC\147.png)

计算公式

![](PIC\148.png)

![](PIC\149.png)

连续型计算步骤：

![](PIC\150.png)

**泊松分布**

> 期望和方差相同
>

![](PIC\151.png)

**均匀分布**

![](PIC\152.png)

**指数分布**

![](PIC\153.png)

#### 性质

![](PIC\154.png)

![155](PIC\155.png)

**<u>只有X和Y相互独立，才有和的方差等于方差的和</u>**

![](PIC\156.png)

**正态分布**

![](PIC\157.png)

![](PIC\158.png) 

![](PIC\159.png)

**正态分布扩展**：

![](PIC\160.png)

## 马尔可夫不等式

描述的是**<u>非负随机变量绝对位置的概率上限</u>**

![](PIC\342.png)

证明：

![](PIC\343.png)

## 切比雪夫不等式

​	切比雪夫：1821-1894，**俄国数学家**

**理论证明和实际应用往往需要估计某些事件发生的概率**

![](PIC\161.png)

**证明：**

1.**马尔科夫不等式的X变形**：

![](PIC\344.png)

**<u>2.利用两次放缩证明不等式</u>**

![](PIC\162.png)

## 协方差与相关系数

**都是刻画描述两个随机变量之间的某种线性关系的特征**

**相关系数：为了把协方差的<u>*量纲*</u>去掉（例如X代表体重，Y代表身高）。也可以说相关系数描述X和Y*线性关系强弱*的量，因此也叫做<u>线性相关系数</u>**

如果X和Y相互独立，协方差为0

![](PIC\163.png)

### 性质

![](PIC\164.png)

![](PIC\165.png)

![166](PIC\166.png)

![](PIC\167.png)

![](PIC\168.png)

**相关系数的性质**

**<u>相关系数 = 1：X和Y之间必然（概率等于1）有着线性关系</u>**

![](PIC\169.png)

![](PIC\170.png)

可以利用**E(XY) = E(X) *E(Y)**来证明不相关

> **独立一定不相关，不相关不一定相互独立**，**例外，二维正态分布是充要条件**

![](PIC\171.png)

## 矩

**一阶原点矩就是X的期望**

![](PIC\172.png)

**二阶中心矩就是X的方差**

![](C:\CHARLES\Personal\专业\数学\概率论\PIC\173.png)

![](PIC\174.png)

![175](PIC\175.png)

## 协方差矩阵

排成矩阵的形式就是**随机变量(X1，X2)的协方差矩阵**

![](PIC\176.png)

![](PIC\177.png)

**例：正态分布**

![](PIC\178.png)

![](PIC\179.png)

![](PIC\180.png)

## 大数定律

![](PIC\181.png)

### 弱大数定理（辛钦大数定理）

**当n个随机变量相互<u>独立，同分布</u>且期望E相等时，<u>*n个随机变量的算术平均依概率收敛于他们的期望E*</u>**

![](PIC\182.png)

**证明：<u>切比雪夫不等式 + 夹逼定理</u>**

![](PIC\183.png)

#### 伯努利大数定理

**事件A发生的频率   依概率->  事件A一次实验中发生的概率**

![](PIC\184.png)

**说明：n充分大的时候，事件的频率与概率偏差小于e（例：抛硬币）**

实际应用中：**当试验次数很大的时候，可以<u>*用事件的频率代替概率*</u>**

![](PIC\185.png)

## 中心极限定理

![](PIC\227.png)

![](PIC\228.png)

### 标准化变量

![](PIC\188.png)

**<u>*作用：n个独立同分布随机变量之和，在n充分大的时候可以近似为标准正态分布*</u>**

当n趋于无穷大，**<u>近似为标准正态分布</u>**

![](PIC\186.png)

![](PIC\187.png)

![](PIC\189.png)

### 棣莫弗-拉普拉斯定理

**<u>作用：中心极限定理在二项分布下的推广，即二项分布以正态分布为其极限分布定律</u>。**

![](PIC\190.png)



![](PIC\191.png)

例：

![](PIC\193.png)

![](PIC\194.png)

![](PIC\195.png)

![196](PIC\196.png)

# **数理统计**

## 概率与统计的区别

概率（probabilty）和统计（statistics）看似两个相近的概念，其实**研究的问题刚好相反**。

概率研究的问题是，已知一个模型和参数，怎么去**预测这个模型产生的结果的特性**（例如均值，方差，协方差等等）。 

统计研究的问题则相反。统计是，有一堆数据，要**利用这堆数据去预测模型和参数。仍以猪为例。**

总结：**概率是已知模型和参数，推数据特性。统计是已知数据特性，推模型和参数。**

## 随机样本

![](PIC\197.png)

**基本概念**

![](PIC\198.png)

![](PIC\199.png)

- **个体 -> 随机变量的每个可能取值**
- **总体 -> 随机变量**

**样本**：总体中抽取出部分的个体

![](PIC\200.png)

![](PIC\201.png)

![](PIC\202.png)

总结：

> 试验**全部可能的观察值为总体**
>
> **每个观察值为个体**
>
> 一个总体对应一个随机变量X，**<u>随机变量服从什么分布，总体就服从什么分布</u>**。
>
> 相同条件下对总体进行**<u>n次重复独立的观察得到的n个结果称为来自总体的简单随机样本</u>**，其与总体具有相同的分布且相互独立（独立同分布）。因此**<u>*本质是用来自样本的信息推断总体*</u>**。

抽样合理的话，【随机】平均数趋于总体的平均数（在平均值附近）。对于**抽样必须保证随机性**，确保每个样本有同样的被抽到的机会（概率相等），在这种条件下的抽样一般是可信的

### 幸存者偏差

通过样本去得到具有**一般性、客观性的规律总结**的一个前提条件是**<u>“样本是随机的”</u>**，这意味着样本的选择排除了主观性的干扰，**得到的样本是“随机变量”**。

例如，返航的战斗机弹痕主要集中在机翼部位，因此有人主张应该加固机翼装甲，统计学家则指出战毁的飞机根本飞不回来，没有弹孔的地方才是致命伤。从统计学上来看，**“幸存者偏差”正是<u>忽略了样本的随机性</u>所导致的结果偏误**。

## 直方图

1. 找出数据的最大值最小值
2. 确定组数、组距、组限
3. 绘制频数分布表
4. 绘制频率直方图

![](PIC\204.png)

![](PIC\205.png)

![](PIC\206.png)

## 统计量

**定义：不含未知参数的关于样本的函数**

![](PIC\207.png)

**样本平均值**

蕴含着**<u>总体的期望信息</u>**

![](PIC\208.png)

**样本方差**

![](PIC\209.png)

**样本标准差**

![](PIC\210.png)

**样本K阶原点矩**

![](PIC\211.png)

**样本k阶中心矩**

![](PIC\212.png)

## 经验分布函数

**总体分布函数的<u>近似估计</u>**

![](PIC\213.png)

![](PIC\214.png)

## 三大抽样分布

抽样分布：统计量的分布

### 卡方分布

![](PIC\215.png)

![](PIC\216.png)

![217](PIC\217.png)

**自由度n：表示所含独立变量的个数是n**

![](PIC\218.png)

**可加性**

![](PIC\219.png)

**期望和方差**

![](PIC\220.png)

![](PIC\221.png)

### t分布

性质

- **当自由度n趋近于无穷，近似标准正态分布**
- 关于t=0对称
- 

![](PIC\222.png)

![](PIC\223.png)

### F分布

![](PIC\224.png)

## 分位点

![](PIC\229.png)

**定义：**

![](PIC\230.png)

**正态分布**

![231](PIC\231.png)

**卡方分布**

![](PIC\232.png)

## 样本均值与样本方差

数字特征

样本均值的期望 = 总体的期望

样本均值的方差  =  总体的方差/样本容量

样本方差的期望 = 总体的方差

![](PIC\233.png)

### 正态总体

![](PIC\234.png)

## 点估计

- **点估计：利用统计量及样本给出期望的一个近似值**
- **区间估计：给出一个区间包含均值的可信程度**
- **假设检验：先给出一个假设，然后利用得到的样本对假设进行判断**

**本质：<u>借助总体X的一个样本来估计总体中未知参数的值</u>**

![](PIC\237.png)

![](PIC\238.png)

例：

![](PIC\235.png)

![](PIC\236.png)

## 矩估计

本质：**<u>*样本的k阶矩依概率收敛于总体的k阶矩*</u>**

![](PIC\239.png)

![](PIC\240.png)

**矩估计求解步骤：**

![](PIC\241.png)

![](PIC\242.png)

总体X的分布中含有m个未知参数

1. **建立m个方程**
2. **求解出矩估计量**

通解：

![](PIC\247.png)

![](PIC\246.png)

**例题：**

![](PIC\244.png)

![](PIC\245.png)

![](PIC\248.png)

![](PIC\249.png)

## 最大似然估计 MLE

（Maximum likelihood estimation, 简称MLE）

> **似然函数：它描述对于不同的模型参数，出现x这个样本点的概率是多少**。

最大似然估计是**<u>求参数θ, 使似然函数最大</u>** 。 最大后验概率估计则是想求最大。最大后验概率估计则是想求最大。**<u>*数据集越大，MLE 估计越准确*</u>**。

通俗理解来说，**<u>就是利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值！</u>**

![](PIC\250.png)

思想

![251](PIC\251.png)

解题步骤：

1.  写出似然函数；
2.  如果无法直接求导的话，对似然函数取对数；
3. 求导数 ；
4. 求解模型中参数的最优值。

![](PIC\252.png)

**取对数方便后面求偏导求极值**

![](PIC\253.png)

![](PIC\254.png)

![](PIC\256.png)

例题：

1.

![](PIC\257.png)

![](PIC\258.png)

![](PIC\259.png)

2.

**矩估计值**

![](PIC\260.png)

![](PIC\261.png)

**最大似然估计值**

![](PIC\262.png)

## 最大后验概率估计 MAP

（Maximum a posteriori estimation, 简称MAP）

最大后验估计是根据经验数据获得对难以观察的量的点估计。与最大似然估计类似，但是最大的不同时，最大后验估计的**<u>融入了要估计量的先验分布在其中</u>**。故最大后验估计可以看做规则化的最大似然估计。

> 反映的是在给定观测数据的基础上，我们对于参数的新的认知。说的更直白一点，就是最开始没有观测数据的时候，我们依据以往的经验赋予了参数一个先验分布，然后来了实际的观测数据之后，我们就对先验进行了更新，得到了这次分析过程的后验分布。

![](PIC\345.png)



## 置信区间

**区间估计**：由于点估计不能反映估计的精确程度。区间估计给出一个范围，并给出**<u>此范围包含参数真值的可信程度</u>**

**区间越短，精确度越高**

![](PIC\263.png)

![](PIC\264.png)

![](PIC\265.png)

**含义：**

![](PIC\266.png)

例：

![](PIC\267.png)

![](PIC\270.png)

核心思想：**利用样本均值和置信区间得到关于未知参数的不等式，然后求解即可**

![](PIC\268.png)

![](PIC\269.png)

![](PIC\271.png)

注意：**相同置信水平的置信区间不唯一，即落在不同区间的可信程度可能是相同的。<u>置信区间越短，精确度越高</u>**

求解方法总结：

1. 寻找**<u>枢轴量W</u>**，即**不依赖于未知参数的含有样本的函数**
2. 对给定的置信水平，通过枢轴量**反解关于未知参数的不等式**，解出两个常数a，b（一般是上分位点）,使得W在(a,b)的概率是 1- 置信水平

![](PIC\272.png)

![](PIC\273.png)

**根据未知参数选取合适的枢轴量**

![](PIC\274.png)

### 正态分布均值与方差的区间估计

### 单个正态分布

![](PIC\275.png)

不同未知参数的情况：

#### 总体均值

1.总体方差已知，利用**样本均值 和 总体均值**构造枢轴量

![](PIC\276.png)

2.总体方差未知，利用样本标准差和**t分布**

![](PIC\277.png)

![](PIC\278.png)

![](PIC\279.png)

#### 总体方差/标准差

利用**卡方分布**

![](PIC\283.png)

> **如果是求标准差的置信区间的话，不等式的方差变为标准差（开根号）即可**

![](PIC\284.png)

例题：

**1.总体均值的置信区间**

![](PIC\280.png)

![](PIC\281.png)

![](PIC\282.png)

**2.总体方差的置信区间**

![](PIC\285.png)

![286](PIC\286.png)

![](PIC\287.png)

#### 总结

![](PIC\308.png)

### 两个正态分布

![](PIC\288.png)

![](PIC\289.png)

#### 均值差的置信区间

![](PIC\290.png)

![](PIC\291.png)

![](PIC\292.png)

**两个总体方差相等但未知**

![](PIC\293.png)

![](PIC\294.png)

![295](PIC\295.png)

> ![](PIC\301.png)

例1：

![](PIC\296.png)

![](PIC\297.png)

![](PIC\298.png)

例2：

![](PIC\299.png)

![](PIC\300.png)

![](PIC\302.png)

#### 方差比的置信区间

![](PIC\303.png)

![](PIC\304.png)

例：

![](PIC\305.png)

![](PIC\307.png)

## 单侧置信区间

**定义：**

![](PIC\309.png)

![](PIC\310.png)

**小结**

![](PIC\311.png)

## **假设检验**

统计推断就是利用样本对整体做一个推断

步骤：

1. **问题**是什么？
2. **证据**是什么？
3. **判断标准**是什么？
4. **结论**

**估计问题**

- **点估计：给出<u>待估参数的近似值</u>**
- **区间估计：给出包含待估参数的可能落入的范围，以及这个范围包含这个参数的可信程度**

![](PIC\312.png)

**假设检验问题**

> **置信区间**:目的是**根据样本构造一个区间，然后希望这个区间可以把真值包含进去**，但是并不知道这个真值是多少
>
> **假设检验**:**假设真值是多少，然后检验这个假设是否可能为真**。

显著性检验是假设检验中最常用的一种方法，也是一种最基本的统计推断形式，其基本原理**<u>*是先对总体的特征做出某种假设，然后通过抽样研究的统计推理，对此假设应该被拒绝还是接受做出推断*</u>**

1. **提出关于总体的某些假设**
2. **然后根据样本提供的信息，<u>决策对于这个假设是接受还是拒</u>绝**

> **“备择假设”**对应的是**“拒绝域”**
>
> **“原假设**”对应的是**“接受域”**
>
> **<u>“拒绝域”有“充分性”，而“接受域”没有“充分性”。</u>**
>
> 这就是说当统计量落入“拒绝域”里时，我们**有充分的理由**拒绝“原假设”而接受“备择假设”；但当统计量落入“接受域”里时，我们**没有充分的理由**拒绝“备择假设”接受“原假设”，只能说是**无法拒绝**“原假设”，更通俗的讲就是**可以勉强接受**“原假设”。

分为两类：

- **参数检验**
- **非参数检验**

![313](PIC\313.png)

![](PIC\314.png)

我们**通过样本数据来判断总体参数的假设**是否成立，但**样本是随机的，因而有可能出现小概率的错误。这种错误分两种：**

1.   	**弃真错误**也叫第I类错误或α错误：它是指<u>**原假设实际上是真的，但通过样本估计总体后，拒绝了原假设**</u>。明显这是错误的，我们**<u>*拒绝了真实的原假设*</u>**，所以叫弃真错误，这个错误的概率我们记为α。这个值也是显著性水平，在假设检验之前我们会规定这个概率的大小。
2.     **取伪错误**也叫第II类错误或β错误：它是指**<u>原假设实际上假的，但通过样本估计总体后，接受了原假设</u>**。明显者是错误的，我们**<u>*接受的原假设实际上是假的*</u>**，所以叫取伪错误，这个错误的概率我们记为β。

做题步骤：

- 提出**<u>零假设H0</u>**和备择假设

- 构造**检验统计量**，并在H0成立的条件下**确定其分布**

- 设置**<u>显著性水平α（一般为0.05）</u>**，在H0成立的条件下根据不等式确定拒绝域和临界点**

- 计算统计量，并**通过统计量获取P值**

- **如果p < = α，那么拒绝零假设，也就是备选假设成立。**

  **如果p > α，那么零假设成立**

> **零假设和备择假设在逻辑方面是互补的**，也就是说，如果其中一个假设为真，则另一个假设为假；如果我们**<u>推翻了其中一个假设，那就必须承认另一个假设</u>**。
>
> **显著性水平α**：通俗来说是**人为设定小概率事件发生（总体统计量分布）的临界值**
>
> **<u>福尔摩斯说：一旦排除所有的不可能，剩下的不管多么难以置信，一定就是真相！</u>**

### P值

*P*值（P-value）就是**当零假设为真时，比所得到的样本观察结果更极端的结果出现的概率和**。如果P 值很小，说明原假设情况的发生的概率很小，而如果出现了，根据小概率原理，我们就有理由拒绝原假设，P值越小，我们拒绝原假设的理由越充分。总之，**<u>P值越小，表明结果越显著</u>**。

    Z 值代表**随机变量经过列维-林德伯格中心极限定理的变形后，服从标准正态分布Φ（0,1），并且Z为该标准正态分布下的新变量**。在数量上表示该新变量为该标准正态分布下标准差σ=1的倍数。

我们一般认为

​	**p-value <= 0.05**

就可以认为零假设是不正确的，当**P-Value的值大于α时，表示支持原假设**。

0.05这个标准就是**显著性水平**，当然选择多少作为显著水平也是主观的。

![](PIC\330.png)

### 单边检测

关注的不是总体参数等于假设参数值，而是**总体参数大于（左边）或小于（右边）假设参数值**

![](PIC\331.png)

例：

![](PIC\315.png)

**1.提出假设**

![](PIC\316.png)

**2.两类错误**

![](PIC\317.png)

**3.显著性检验**

![](PIC\318.png)

![](PIC\319.png)

**4.找出正数k**

![](PIC\320.png)

分类：

![](PIC\321.png)

例：

![](PIC\322.png)

![](PIC\323.png)

![](PIC\324.png)

1.关于**均值**的假设检验：**Z检验，T检验**

![](PIC\332.png)

#### Z检验

**适用条件**

- 正态分布
- 总体标准差已知或者样本容量足够大(>30)

 **用途**

- **检验一个样本平均数与一个己知的总体平均数的差异是否显著**
- 检验来自两个的两组样本平均数的差异性，从而判断它们各自代表的总体的差异是否显著

![](PIC\333.png)

#### T检验

> 从抽样研究所得的样本均数特点来看，只要**样本量>60，（无论总体是否服从正态分布）抽样研究的样本均数服从或者近似服从正态分布**；
>
> 而如果样本量较小（参考样本量<100）,**<u>抽样分布随着样本量的减小，与正态分布的差别越来越大</u>**。此时需要用**小样本理论来解释样本均数的分布——而t分布就是小样本理论的代表**、

![](PIC\334.png)

用途

- **样本均数与总体均数的比较看差异是否显著**；
- 两样本均数的比较看差异是否显著。

![](PIC\335.png)

2.有关参数**方差**的假设检验：**F检验**

![](PIC\336.png)

**适用条件**

- 总体均值未知
- 样本来自于正态总体

**用途**

- **比较两组数据的方差，以确定他们的精密度是否有显著性差异**
- 在进行双独立样本T检验时，对于两组小样本，需进行F检验以测试其方差齐性

3.两个或多个变量之间是否有关系：**卡方检验**

![337](PIC\337.png)

 **用途**

用于检验**两个变量之间有没有关系**。

- **卡方检验可以检验男性或者女性对线上买生鲜食品有没有区别**；
- **不同城市级别的消费者对买SUV车有没有什么区别**；

![](PIC\337.png)

![338](PIC\338.png)